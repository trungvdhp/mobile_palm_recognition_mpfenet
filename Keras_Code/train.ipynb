{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T08:23:51.488757Z",
     "start_time": "2021-10-31T08:17:03.028301Z"
    },
    "code_folding": [
     20,
     24,
     27,
     37,
     57,
     84,
     88,
     99,
     113,
     124,
     226,
     231
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training databases.\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\tongji**/*.tiff\n",
      "Num images: 12000\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\polyu**/*.tiff\n",
      "Num images: 6000\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\casia**/*.tiff\n",
      "Num images: 1200\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\xjtu_up**/*.tiff\n",
      "Num images: 3920\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\ntust_hp**/*.tiff\n",
      "Num images: 2180\n",
      "D:\\mobile_palm_vein_recognition_mpsnet\\Keras_Code\\data\\ntust_ip**/*.tiff\n",
      "Num images: 5760\n",
      "Total training images:  31060\n",
      "Load test databases.\n",
      "Total test images:  2180\n",
      "Fitting fold #1...\n",
      "WARNING:tensorflow:Entity <bound method SpatialPyramidPooling.call of <model.spp.SpatialPyramidPooling object at 0x0000021576E31550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SpatialPyramidPooling.call of <model.spp.SpatialPyramidPooling object at 0x0000021576E31550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method SpatialPyramidPooling.call of <model.spp.SpatialPyramidPooling object at 0x0000021576E31550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method SpatialPyramidPooling.call of <model.spp.SpatialPyramidPooling object at 0x0000021576E31550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"mpsnet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   288         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 64, 64, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   2048        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseConv (None, 32, 32, 64)   576         re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         depthwise_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 32)   2048        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 64)   2048        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 32, 32, 64)   576         re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   2048        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 32, 32, 32)   1312        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 32)   128         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 32)   0           batch_normalization_7[0][0]      \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 16, 16, 64)   576         re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   4096        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  8192        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 16, 16, 128)  1152        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 64)   8192        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 16, 16, 64)   2336        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 192)  12288       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 192)  768         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 16, 16, 192)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 8, 8, 192)    1728        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 192)    768         depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 8, 8, 192)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 64)     12288       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 192)    12288       batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 192)    768         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 8, 8, 192)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_5 (DepthwiseCo (None, 8, 8, 192)    1728        re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 192)    768         depthwise_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 8, 8, 192)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 64)     12288       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 64)     0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 192)    12288       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 192)    768         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 8, 8, 192)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_6 (DepthwiseCo (None, 8, 8, 192)    1728        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 192)    768         depthwise_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 8, 8, 192)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     12288       re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 8, 8, 64)     4672        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 64)     256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 64)     0           batch_normalization_23[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 64)     0           batch_normalization_24[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 128)    8192        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_7 (DepthwiseCo (None, 8, 8, 128)    1152        re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         depthwise_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 128)    16384       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 256)    32768       batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_8 (DepthwiseCo (None, 8, 8, 256)    2304        re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 256)    1024        depthwise_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 128)    32768       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 128)    8768        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 128)    512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 128)    0           batch_normalization_30[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 128)    0           batch_normalization_31[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8, 8, 128)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_pyramid_pooling (Spatia (None, 2688)         0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2688)         10752       spatial_pyramid_pooling[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 1, 2688)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 1, 128)    344064      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 1, 1, 128)    512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 1, 1, 2002)   256256      batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 2002)         0           conv2d_20[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,120\n",
      "Trainable params: 836,448\n",
      "Non-trainable params: 12,672\n",
      "__________________________________________________________________________________________________\n",
      "First phase: Fitting model with Softmax loss...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24848 samples, validate on 6212 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "24840/24848 [============================>.] - ETA: 0s - loss: 13.6067 - acc: 0.0017\n",
      "Epoch 00001: val_loss improved from inf to 11.71814, saving model to result\\session_1\\mpsnet\\softmax_fold1.hdf5\n",
      "24848/24848 [==============================] - 113s 5ms/sample - loss: 13.6072 - acc: 0.0017 - val_loss: 11.7181 - val_acc: 8.0489e-04\n",
      "Epoch 2/50\n",
      "24840/24848 [============================>.] - ETA: 0s - loss: 11.9150 - acc: 6.0386e-04\n",
      "Epoch 00002: val_loss improved from 11.71814 to 10.03074, saving model to result\\session_1\\mpsnet\\softmax_fold1.hdf5\n",
      "24848/24848 [==============================] - 83s 3ms/sample - loss: 11.9144 - acc: 6.0367e-04 - val_loss: 10.0307 - val_acc: 1.6098e-04\n",
      "Epoch 3/50\n",
      "24840/24848 [============================>.] - ETA: 0s - loss: 12.6976 - acc: 5.2335e-04\n",
      "Epoch 00003: val_loss did not improve from 10.03074\n",
      "24848/24848 [==============================] - 82s 3ms/sample - loss: 12.6981 - acc: 5.2318e-04 - val_loss: 11.8852 - val_acc: 6.4392e-04\n",
      "Epoch 4/50\n",
      "15432/24848 [=================>............] - ETA: 29s - loss: 11.4843 - acc: 4.5360e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\optimizers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[0mtest_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_sessions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretrain_softmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrain_fine_tune\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_to_tflite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_sessions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_sessions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m     \u001b[0mretrain_softmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[0mretrain_finetune\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\api\\_v1\\keras\\optimizers\\__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(retrain_softmax, retrain_fine_tune, convert_to_tflite, n_sessions)\u001b[0m\n\u001b[0;32m    279\u001b[0m                                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarmup_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m                                                 \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m                                                 callbacks=[checkpoint])\n\u001b[0m\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[0mhistory_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostfix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_history.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data.dataloader import DataLoader\n",
    "from options import Options\n",
    "from model.nets import Net\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "try:\n",
    "    tf_gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in tf_gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def get_optimizer(optimizer_name, lr):\n",
    "    if(optimizer_name == 'rmsprop'):\n",
    "        return RMSprop(lr=lr)\n",
    "    elif(optimizer_name == 'adam'):\n",
    "        return Adam(lr=lr)\n",
    "    elif(optimizer_name == 'adadelta'):\n",
    "        return Adadelta(lr=lr)\n",
    "    elif(optimizer_name == 'sgd'):\n",
    "        return SGD(lr=lr)\n",
    "        \n",
    "def extract_genuines_impostors_1(distances,labels,sort=True):\n",
    "    \n",
    "    num_features = distances.shape[0]\n",
    "    genuines = []\n",
    "    impostors = []\n",
    "\n",
    "    for i in range(num_features-1):\n",
    "        \n",
    "        for j in range(i+1, num_features):\n",
    "            \n",
    "            if(labels[i]==labels[j]):\n",
    "                genuines.append(distances[i, j])\n",
    "            else:\n",
    "                impostors.append(distances[i, j])\n",
    "    if sort:\n",
    "        genuines=sorted(genuines)\n",
    "        impostors=sorted(impostors)\n",
    "        \n",
    "    return np.array(genuines), np.array(impostors)\n",
    "\n",
    "def extract_genuines_impostors_2(distances,class_size=20, sort=True):\n",
    "    # Get genuine matching and impostor matching scores\n",
    "    num_columns = distances.shape[0]\n",
    "    num_blocks = num_columns//class_size\n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    half_class_size = class_size//2\n",
    "\n",
    "    for block_id in range(num_blocks):\n",
    "        start = class_size*block_id\n",
    "        end = start+half_class_size\n",
    "        \n",
    "        for i in range(start, end, 1):\n",
    "            for j in range(end, end+half_class_size, 1):\n",
    "                genuine_scores.append(distances[i, j])\n",
    "                \n",
    "            for j in range(half_class_size, num_columns, class_size):\n",
    "                if j != end:\n",
    "                    for k in range(j, j+half_class_size,1):\n",
    "                        impostor_scores.append(distances[i, k])\n",
    "    \n",
    "    if sort:\n",
    "        genuine_scores=sorted(genuine_scores)\n",
    "        impostor_scores=sorted(impostor_scores)\n",
    "        \n",
    "    return np.array(genuine_scores), np.array(impostor_scores)\n",
    "\n",
    "def plot_legend(loc):\n",
    "    legend = plt.legend(loc=loc, shadow=False, prop={'size': 10})\n",
    "    legend.get_frame().set_facecolor('#ffffff')  \n",
    "    \n",
    "def plot_DET(frr, far, linthresh, output_path):\n",
    "    plt.figure()\n",
    "    scale_type='symlog'\n",
    "    plt.xscale(scale_type, linthresh=linthresh)\n",
    "    plt.yscale(scale_type, linthresh=linthresh)\n",
    "    plt.plot(frr, far, linestyle='-', linewidth=1, label=config.model_name)\n",
    "    plt.xlabel('False Rejected Rate(%)')\n",
    "    plt.ylabel('False Accepted Rate(%)')\n",
    "    plot_legend('best')\n",
    "    plt.savefig(output_path + '_DET.png')\n",
    "\n",
    "def plot_MDD(genuines, impostors, output_path):\n",
    "    # Produce matching distance distributions\n",
    "    df1 = pd.DataFrame(genuines, columns = ['GenuineScores'])\n",
    "    df1.GenuineScores.plot.kde(label='Genuine')\n",
    "    df2 = pd.DataFrame(impostors, columns = ['ImpostorScores'])\n",
    "    df2.ImpostorScores.plot.kde(label='Impostor')\n",
    "    \n",
    "    # Plot matching distance distributions\n",
    "    plt.figure()\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plot_legend('best')\n",
    "    plt.savefig(output_path + 'MDD.png')\n",
    "    \n",
    "def convert_to_TFLite(keras_model, file_path=''):\n",
    "    \n",
    "    print(\"Converting to TFLite...\", end='\\r')\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    open(file_path,'wb').write(tflite_model)\n",
    "    print(\"Converted to TFLite.\")\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "def test(model, fold, n_sessions=1):\n",
    "    \n",
    "    fname = dataloader.test_name + '_fold' + str(fold)\n",
    "    fpath = config.output_folder + fname\n",
    "    feature_path = fpath + \".hdf5\"\n",
    "    \n",
    "    print(\"Evaluate fold %i on %s database:\"%(fold, dataloader.test_name))\n",
    "    print(\"Get features.\")\n",
    "    \n",
    "    if(model!=None):\n",
    "\n",
    "        inputs  = model.inputs[0]\n",
    "        outputs = model.outputs\n",
    "        get_output = K.function(inputs,outputs)\n",
    "        n = outputs[0].shape[-1]\n",
    "        features = np.zeros((dataloader.n_test_samples, n), dtype=\"float32\")\n",
    "        labels = dataloader.test_labels\n",
    "\n",
    "        for i in range(0, dataloader.n_test_samples, dataloader.test_class_size):\n",
    "            j = i+dataloader.test_class_size\n",
    "            batch = dataloader.test_data[i:j]\n",
    "            fs = np.array(get_output(batch))[0]\n",
    "            features[i:j] = fs\n",
    "       \n",
    "        # save features and labels\n",
    "        with h5py.File(feature_path, 'w') as h5f_data:\n",
    "            h5f_data.create_dataset(\"features\", data=features, maxshape=(None, features.shape[1]), chunks=True)\n",
    "            h5f_data.create_dataset(\"labels\", data=dataloader.test_labels, maxshape=(None,), chunks=True) \n",
    "    else:\n",
    "        with h5py.File(feature_path, 'r') as h5f_data:\n",
    "            features = np.array(h5f_data[\"features\"])\n",
    "            labels = np.array(h5f_data[\"labels\"])\n",
    "    \n",
    "    # Verification\n",
    "    print (\"Verification.\")\n",
    "    distances = pairwise_distances(features, Y=None, metric=config.distance_metric)\n",
    "    \n",
    "    if(n_sessions==1):\n",
    "        genuines, impostors = extract_genuines_impostors_1(distances, labels, True)\n",
    "    else:\n",
    "        genuines, impostors = extract_genuines_impostors_2(distances, dataloader.test_class_size, True)\n",
    "    far = []\n",
    "    frr = [] \n",
    "\n",
    "    # generate thresholds\n",
    "    n_thresholds = 1000\n",
    "    epsilon = 1e-5\n",
    "    start = min(0, np.amin(genuines)-epsilon)\n",
    "    end = np.amax(impostors)+epsilon\n",
    "    threshold_step = (end-start)/n_thresholds\n",
    "    thresholds=np.arange(start, end, threshold_step)\n",
    "    best_frr_pos=0\n",
    "\n",
    "    n_genuines = len(genuines)\n",
    "    n_impostors = len(impostors)\n",
    "    n_thresholds = len(thresholds)\n",
    "    \n",
    "    # for each threshold, calculate confusion matrix.\n",
    "    for k in range(n_thresholds):\n",
    "\n",
    "        t = thresholds[k]\n",
    "\n",
    "        FP = np.searchsorted(impostors, t, side='right')\n",
    "        FN = n_genuines - np.searchsorted(genuines, t, side='right')\n",
    "\n",
    "        far_current = 100.0 * (float(FP) / float(n_impostors))\n",
    "        frr_current = 100.0 * (float(FN) / float(n_genuines))\n",
    "        far.append(far_current)\n",
    "        frr.append(frr_current)\n",
    "\n",
    "    # calculate the most optimal FAR and FRR values\n",
    "    f = np.abs(np.array(frr)-np.array(far))\n",
    "    k = np.argmin(f)\n",
    "\n",
    "    eer = (far[k]+frr[k])/2\n",
    "    eer_threshold=thresholds[k]\n",
    "\n",
    "    # write verification scores to file\n",
    "    scores_path  = fpath + '_scores.txt'\n",
    "    \n",
    "    with open(scores_path, 'w') as file:\n",
    "\n",
    "        file.write(\"\\nID: {:d}, Threshold: {:.3f}, FRR: {:.3f}, FAR: {:.3f}, EER: {:.3f}\\n\\n\".format(k+1, \n",
    "                                                                                                 eer_threshold,\n",
    "                                                                                                 frr[k],far[k], \n",
    "                                                                                                 eer))\n",
    "        file.write(\"{:3s} {:12s} {:12s} {:12s}\\n\\n\".format(\"ID\", \"Thresholds\", \"FRR\", \"FAR\"))\n",
    "\n",
    "        for x in zip(range(1,n_thresholds+1), thresholds, frr, far):\n",
    "            file.write(\"{:d} {:12.6f} {:12.6f} {:12.6f}\\n\".format(*x))\n",
    "\n",
    "    print('Verification results:')\n",
    "    print(\"ID: %i, Threshold: %.3f, FRR: %.3f, FAR: %.3f, EER: %.3f\"%(k+1, eer_threshold, frr[k], far[k], eer))\n",
    "    \n",
    "    # Plot DET\n",
    "    plot_DET(frr, far, linthresh=30, output_path=fpath)\n",
    "    \n",
    "    # Plot matching distance distributions\n",
    "    plot_MDD(genuines, impostors, output_path=fpath)\n",
    "    \n",
    "    return eer\n",
    "\n",
    "def train(\n",
    "    retrain_softmax=True, \n",
    "    retrain_fine_tune=True,\n",
    "    convert_to_tflite=True,\n",
    "    n_sessions=1\n",
    "):\n",
    "    best_eer = 100.0\n",
    "    best_fold = 1\n",
    "    avg_eer = 0.0\n",
    "    net = Net(config)\n",
    "    \n",
    "    for fold in range(config.n_folds):\n",
    "        train_data, train_labels, valid_data, valid_labels = dataloader.get_fold_data(fold)\n",
    "        fold+=1\n",
    "        \n",
    "        if(retrain_softmax==False and retrain_fine_tune==False):\n",
    "            net.adacos_model = None\n",
    "        else:\n",
    "         \n",
    "            print('Fitting fold #%i...'%fold)\n",
    "            K.clear_session()\n",
    "            train_labels_float = train_labels.astype(float)\n",
    "            valid_labels_float = valid_labels.astype(float)\n",
    "            \n",
    "            # Get model\n",
    "            if(config.model_name=='mpsnet'):\n",
    "                net.build_mpsnet_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v1'):\n",
    "                net.build_mobilenet_v1_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v2'):\n",
    "                net.build_mobilenet_v2_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v3'):\n",
    "                net.build_mobilenet_v3_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilefacenet'):\n",
    "                net.build_mobilefacenet_backbone(input_shape = dataloader.sample_shape)\n",
    "                \n",
    "            net.build_softmax_model(n_classes=dataloader.n_train_classes)\n",
    "            optimizer = get_optimizer(config.warmup_optimizer, config.warmup_lr)\n",
    "            net.softmax_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "            postfix = config.output_folder + 'softmax_fold' + str(fold)\n",
    "            best_weight = postfix + '.hdf5'\n",
    "\n",
    "            if(retrain_softmax):\n",
    "                net.softmax_model.summary()\n",
    "                \n",
    "                print(\"First phase: Fitting model with Softmax loss...\\n\")\n",
    "                \n",
    "                checkpoint = ModelCheckpoint(best_weight, verbose=1, save_best_only=True)\n",
    "                \n",
    "                history = net.softmax_model.fit(train_data, train_labels_float, \n",
    "                                                validation_data=(valid_data, valid_labels_float), \n",
    "                                                epochs=config.warmup_epochs, \n",
    "                                                batch_size=config.warmup_batch_size,\n",
    "                                                class_weight=dataloader.class_weights,\n",
    "                                                callbacks=[checkpoint])\n",
    "                \n",
    "                history_path = postfix + '_history.pkl'\n",
    "\n",
    "                with open(history_path, 'wb') as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "\n",
    "            if(retrain_fine_tune):\n",
    "                net.softmax_model.load_weights(best_weight)\n",
    "                softmax_valid_scores = net.softmax_model.evaluate(valid_data, valid_labels_float, verbose=0)\n",
    "                print('Fold #%i validation scores (Softmax): '%fold, softmax_valid_scores)\n",
    "\n",
    "            ### Fine tune with AdaCos\n",
    "            postfix = config.output_folder + 'adacos_fold' + str(fold)\n",
    "            best_weight =  postfix + '.hdf5'\n",
    "            \n",
    "            # Get fine tune model\n",
    "            net.build_adacos_model()\n",
    "            optimizer = get_optimizer(config.fine_tune_optimizer, config.fine_tune_lr)\n",
    "            net.adacos_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "            if(retrain_fine_tune):\n",
    "                net.adacos_model.summary()\n",
    "                \n",
    "                print(\"Second phase: Fine tune the best softmax model with AdaCos...\")\n",
    "                \n",
    "                checkpoint = ModelCheckpoint(best_weight, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "                history = net.adacos_model.fit([train_data, train_labels], train_labels_float, \n",
    "                                                batch_size=config.fine_tune_batch_size, \n",
    "                                                epochs=config.fine_tune_epochs, \n",
    "                                                validation_data=([valid_data, valid_labels], valid_labels_float), \n",
    "                                                class_weight=dataloader.class_weights,\n",
    "                                                callbacks=[checkpoint])\n",
    "                history_path = postfix + '_history.pkl'\n",
    "\n",
    "                with open(history_path, 'wb') as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "\n",
    "            net.adacos_model.load_weights(best_weight)\n",
    "            adacos_valid_scores = net.adacos_model.evaluate([valid_data, valid_labels], valid_labels_float, verbose=0)\n",
    "            print('Fold #%i validation scores (AdaCos): '%fold, adacos_valid_scores)\n",
    "        \n",
    "        # Convert model to TensorFlow-Lite version\n",
    "        if(net.adacos_model!=None):\n",
    "            tflite_file_path = postfix + '.tflite'\n",
    "            net.adacos_model = Model(inputs=net.adacos_model.inputs[0], outputs=net.adacos_model.get_layer(config.embedding_layer_name).output)\n",
    "        \n",
    "            if(convert_to_tflite):\n",
    "                convert_to_TFLite(net.adacos_model, tflite_file_path)\n",
    "                \n",
    "        # Get eer\n",
    "        eer = test(net.adacos_model, fold, n_sessions=n_sessions)\n",
    "        \n",
    "        # update eer\n",
    "        avg_eer += eer\n",
    "        \n",
    "        if(eer<best_eer):\n",
    "            best_eer=eer\n",
    "            best_fold=fold\n",
    "\n",
    "    # calculate average eer\n",
    "    avg_eer /= config.n_folds\n",
    "\n",
    "    # write test scores to file\n",
    "    scores_path = config.output_folder + dataloader.test_name + '_scores.txt'\n",
    "    \n",
    "    with open(scores_path, \"w\") as f:\n",
    "        file.write(\"Best fold: {:d}\\nBest EER: {:.6f}\\nAverage EER: {:.6f}\".format(best_fold, best_eer,avg_eer)) \n",
    "    print(\"Best fold: %i\\nBest EER: %.6f\\nAverage EER: %.6f\"%(best_fold, best_eer, avg_eer))\n",
    "\n",
    "config = Options().parse()\n",
    "dataloader = DataLoader(config)\n",
    "retrain_softmax   = True\n",
    "retrain_fine_tune = True\n",
    "convert_to_tflite = True\n",
    "\n",
    "for line in config.test_folders:\n",
    "    parts = line.strip().split(' ')\n",
    "    test_folder, n_sessions = parts[0], int(parts[1])\n",
    "    dataloader.load_test_data(test_folder = test_folder)\n",
    "    train(retrain_softmax, retrain_fine_tune, convert_to_tflite, n_sessions=n_sessions)\n",
    "    retrain_softmax   = False\n",
    "    retrain_finetune  = False\n",
    "    convert_to_tflite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras-GPU",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
