{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T08:23:51.488757Z",
     "start_time": "2021-10-31T08:17:03.028301Z"
    },
    "code_folding": [
     20,
     24,
     27,
     37,
     57,
     84,
     88,
     99,
     113,
     124,
     226
    ]
   },
   "outputs": [],
   "source": [
    "from data.dataloader import DataLoader\n",
    "from options import Options\n",
    "from model.nets import Net\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.argv = ['']\n",
    "\n",
    "try:\n",
    "    tf_gpus = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in tf_gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def get_optimizer(optimizer_name, lr):\n",
    "    if(optimizer_name == 'rmsprop'):\n",
    "        return RMSprop(lr=lr)\n",
    "    elif(optimizer_name == 'adam'):\n",
    "        return Adam(lr=lr)\n",
    "    elif(optimizer_name == 'adadelta'):\n",
    "        return Adadelta(lr=lr)\n",
    "    elif(optimizer_name == 'sgd'):\n",
    "        return SGD(lr=lr)\n",
    "        \n",
    "def extract_genuines_impostors_1(distances,labels,sort=True):\n",
    "    \n",
    "    num_features = distances.shape[0]\n",
    "    genuines = []\n",
    "    impostors = []\n",
    "\n",
    "    for i in range(num_features-1):\n",
    "        \n",
    "        for j in range(i+1, num_features):\n",
    "            \n",
    "            if(labels[i]==labels[j]):\n",
    "                genuines.append(distances[i, j])\n",
    "            else:\n",
    "                impostors.append(distances[i, j])\n",
    "    if sort:\n",
    "        genuines=sorted(genuines)\n",
    "        impostors=sorted(impostors)\n",
    "        \n",
    "    return np.array(genuines), np.array(impostors)\n",
    "\n",
    "def extract_genuines_impostors_2(distances,class_size=20, sort=True):\n",
    "    # Get genuine matching and impostor matching scores\n",
    "    num_columns = distances.shape[0]\n",
    "    num_blocks = num_columns//class_size\n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    half_class_size = class_size//2\n",
    "\n",
    "    for block_id in range(num_blocks):\n",
    "        start = class_size*block_id\n",
    "        end = start+half_class_size\n",
    "        \n",
    "        for i in range(start, end, 1):\n",
    "            for j in range(end, end+half_class_size, 1):\n",
    "                genuine_scores.append(distances[i, j])\n",
    "                \n",
    "            for j in range(half_class_size, num_columns, class_size):\n",
    "                if j != end:\n",
    "                    for k in range(j, j+half_class_size,1):\n",
    "                        impostor_scores.append(distances[i, k])\n",
    "    \n",
    "    if sort:\n",
    "        genuine_scores=sorted(genuine_scores)\n",
    "        impostor_scores=sorted(impostor_scores)\n",
    "        \n",
    "    return np.array(genuine_scores), np.array(impostor_scores)\n",
    "\n",
    "def plot_legend(loc):\n",
    "    legend = plt.legend(loc=loc, shadow=False, prop={'size': 10})\n",
    "    legend.get_frame().set_facecolor('#ffffff')  \n",
    "    \n",
    "def plot_DET(frr, far, linthresh, output_path):\n",
    "    plt.figure()\n",
    "    scale_type='symlog'\n",
    "    plt.xscale(scale_type, linthresh=linthresh)\n",
    "    plt.yscale(scale_type, linthresh=linthresh)\n",
    "    plt.plot(frr, far, linestyle='-', linewidth=1, label=config.model_name)\n",
    "    plt.xlabel('False Rejected Rate(%)')\n",
    "    plt.ylabel('False Accepted Rate(%)')\n",
    "    plot_legend('best')\n",
    "    plt.savefig(output_path + '_DET.png')\n",
    "\n",
    "def plot_MDD(genuines, impostors, output_path):\n",
    "    # Produce matching distance distributions\n",
    "    df1 = pd.DataFrame(genuines, columns = ['GenuineScores'])\n",
    "    df1.GenuineScores.plot.kde(label='Genuine')\n",
    "    df2 = pd.DataFrame(impostors, columns = ['ImpostorScores'])\n",
    "    df2.ImpostorScores.plot.kde(label='Impostor')\n",
    "    \n",
    "    # Plot matching distance distributions\n",
    "    plt.figure()\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plot_legend('best')\n",
    "    plt.savefig(output_path + 'MDD.png')\n",
    "    \n",
    "def convert_to_TFLite(keras_model, file_path=''):\n",
    "    \n",
    "    print(\"Converting to TFLite...\", end='\\r')\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "    converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    open(file_path,'wb').write(tflite_model)\n",
    "    print(\"Converted to TFLite.\")\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "def test(model, fold, n_sessions=1):\n",
    "    \n",
    "    fname = dataloader.test_name + '_fold' + str(fold)\n",
    "    fpath = config.output_folder + fname\n",
    "    feature_path = fpath + \".hdf5\"\n",
    "    \n",
    "    print(\"Evaluate fold %i on %s database:\"%(fold, dataloader.test_name))\n",
    "    print(\"Get features.\")\n",
    "    \n",
    "    if(model!=None):\n",
    "\n",
    "        inputs  = model.inputs[0]\n",
    "        outputs = model.outputs\n",
    "        get_output = K.function(inputs,outputs)\n",
    "        n = outputs[0].shape[-1]\n",
    "        features = np.zeros((dataloader.n_test_samples, n), dtype=\"float32\")\n",
    "        labels = dataloader.test_labels\n",
    "\n",
    "        for i in range(0, dataloader.n_test_samples, dataloader.test_class_size):\n",
    "            j = i+dataloader.test_class_size\n",
    "            batch = dataloader.test_data[i:j]\n",
    "            fs = np.array(get_output(batch))[0]\n",
    "            features[i:j] = fs\n",
    "       \n",
    "        # save features and labels\n",
    "        with h5py.File(feature_path, 'w') as h5f_data:\n",
    "            h5f_data.create_dataset(\"features\", data=features, maxshape=(None, features.shape[1]), chunks=True)\n",
    "            h5f_data.create_dataset(\"labels\", data=dataloader.test_labels, maxshape=(None,), chunks=True) \n",
    "    else:\n",
    "        with h5py.File(feature_path, 'r') as h5f_data:\n",
    "            features = np.array(h5f_data[\"features\"])\n",
    "            labels = np.array(h5f_data[\"labels\"])\n",
    "    \n",
    "    # Verification\n",
    "    print (\"Verification.\")\n",
    "    distances = pairwise_distances(features, Y=None, metric=config.distance_metric)\n",
    "    \n",
    "    if(n_sessions==1):\n",
    "        genuines, impostors = extract_genuines_impostors_1(distances, labels, True)\n",
    "    else:\n",
    "        genuines, impostors = extract_genuines_impostors_2(distances, dataloader.test_class_size, True)\n",
    "    far = []\n",
    "    frr = [] \n",
    "\n",
    "    # generate thresholds\n",
    "    n_thresholds = 1000\n",
    "    epsilon = 1e-5\n",
    "    start = min(0, np.amin(genuines)-epsilon)\n",
    "    end = np.amax(impostors)+epsilon\n",
    "    threshold_step = (end-start)/n_thresholds\n",
    "    thresholds=np.arange(start, end, threshold_step)\n",
    "    best_frr_pos=0\n",
    "\n",
    "    n_genuines = len(genuines)\n",
    "    n_impostors = len(impostors)\n",
    "    n_thresholds = len(thresholds)\n",
    "    \n",
    "    # for each threshold, calculate confusion matrix.\n",
    "    for k in range(n_thresholds):\n",
    "\n",
    "        t = thresholds[k]\n",
    "\n",
    "        FP = np.searchsorted(impostors, t, side='right')\n",
    "        FN = n_genuines - np.searchsorted(genuines, t, side='right')\n",
    "\n",
    "        far_current = 100.0 * (float(FP) / float(n_impostors))\n",
    "        frr_current = 100.0 * (float(FN) / float(n_genuines))\n",
    "        far.append(far_current)\n",
    "        frr.append(frr_current)\n",
    "\n",
    "    # calculate the most optimal FAR and FRR values\n",
    "    f = np.abs(np.array(frr)-np.array(far))\n",
    "    k = np.argmin(f)\n",
    "\n",
    "    eer = (far[k]+frr[k])/2\n",
    "    eer_threshold=thresholds[k]\n",
    "\n",
    "    # write verification scores to file\n",
    "    scores_path  = fpath + '_scores.txt'\n",
    "    \n",
    "    with open(scores_path, 'w') as file:\n",
    "\n",
    "        file.write(\"\\nID: {:d}, Threshold: {:.3f}, FRR: {:.3f}, FAR: {:.3f}, EER: {:.3f}\\n\\n\".format(k+1, \n",
    "                                                                                                 eer_threshold,\n",
    "                                                                                                 frr[k],far[k], \n",
    "                                                                                                 eer))\n",
    "        file.write(\"{:3s} {:12s} {:12s} {:12s}\\n\\n\".format(\"ID\", \"Thresholds\", \"FRR\", \"FAR\"))\n",
    "\n",
    "        for x in zip(range(1,n_thresholds+1), thresholds, frr, far):\n",
    "            file.write(\"{:d} {:12.6f} {:12.6f} {:12.6f}\\n\".format(*x))\n",
    "\n",
    "    print('Verification results:')\n",
    "    print(\"ID: %i, Threshold: %.3f, FRR: %.3f, FAR: %.3f, EER: %.3f\"%(k+1, eer_threshold, frr[k], far[k], eer))\n",
    "    \n",
    "    # Plot DET\n",
    "    plot_DET(frr, far, linthresh=30, output_path=fpath)\n",
    "    \n",
    "    # Plot matching distance distributions\n",
    "    plot_MDD(genuines, impostors, output_path=fpath)\n",
    "    \n",
    "    return eer\n",
    "\n",
    "def train(\n",
    "    retrain_softmax=True, \n",
    "    retrain_fine_tune=True,\n",
    "    convert_to_tflite=True,\n",
    "    n_sessions=1\n",
    "):\n",
    "    best_eer = 100.0\n",
    "    best_fold = 1\n",
    "    avg_eer = 0.0\n",
    "    net = Net(config)\n",
    "    \n",
    "    for fold in range(config.n_folds):\n",
    "        train_data, train_labels, valid_data, valid_labels = dataloader.get_fold_data(fold)\n",
    "        fold+=1\n",
    "        \n",
    "        if(retrain_softmax==False and retrain_fine_tune==False):\n",
    "            net.adacos_model = None\n",
    "        else:\n",
    "         \n",
    "            print('Fitting fold #%i...'%fold)\n",
    "            K.clear_session()\n",
    "            train_labels_float = train_labels.astype(float)\n",
    "            valid_labels_float = valid_labels.astype(float)\n",
    "            \n",
    "            # Get model\n",
    "            if(config.model_name=='mpsnet'):\n",
    "                net.build_mpsnet_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v1'):\n",
    "                net.build_mobilenet_v1_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v2'):\n",
    "                net.build_mobilenet_v2_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilenet_v3'):\n",
    "                net.build_mobilenet_v3_backbone(input_shape = dataloader.sample_shape)\n",
    "            elif(config.model_name=='mobilefacenet'):\n",
    "                net.build_mobilefacenet_backbone(input_shape = dataloader.sample_shape)\n",
    "                \n",
    "            net.build_softmax_model(n_classes=dataloader.n_train_classes)\n",
    "            optimizer = get_optimizer(config.warmup_optimizer, config.warmup_lr)\n",
    "            net.softmax_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "            postfix = config.output_folder + 'softmax_fold' + str(fold)\n",
    "            best_weight = postfix + '.hdf5'\n",
    "\n",
    "            if(retrain_softmax):\n",
    "                net.softmax_model.summary()\n",
    "                \n",
    "                print(\"First phase: Fitting model with Softmax loss...\\n\")\n",
    "                \n",
    "                checkpoint = ModelCheckpoint(best_weight, verbose=1, save_best_only=True)\n",
    "                \n",
    "                history = net.softmax_model.fit(train_data, train_labels_float, \n",
    "                                                validation_data=(valid_data, valid_labels_float), \n",
    "                                                epochs=config.warmup_epochs, \n",
    "                                                batch_size=config.warmup_batch_size,\n",
    "                                                class_weight=dataloader.class_weights,\n",
    "                                                callbacks=[checkpoint])\n",
    "                \n",
    "                history_path = postfix + '_history.pkl'\n",
    "\n",
    "                with open(history_path, 'wb') as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "\n",
    "            if(retrain_fine_tune):\n",
    "                net.softmax_model.load_weights(best_weight)\n",
    "                softmax_valid_scores = net.softmax_model.evaluate(valid_data, valid_labels_float, verbose=0)\n",
    "                print('Fold #%i validation scores (Softmax): '%fold, softmax_valid_scores)\n",
    "\n",
    "            ### Fine tune with AdaCos\n",
    "            postfix = config.output_folder + 'adacos_fold' + str(fold)\n",
    "            best_weight =  postfix + '.hdf5'\n",
    "            \n",
    "            # Get fine tune model\n",
    "            net.build_adacos_model()\n",
    "            optimizer = get_optimizer(config.fine_tune_optimizer, config.fine_tune_lr)\n",
    "            net.adacos_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "            if(retrain_fine_tune):\n",
    "                net.adacos_model.summary()\n",
    "                \n",
    "                print(\"Second phase: Fine tune the best softmax model with AdaCos...\")\n",
    "                \n",
    "                checkpoint = ModelCheckpoint(best_weight, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "                history = net.adacos_model.fit([train_data, train_labels], train_labels_float, \n",
    "                                                batch_size=config.fine_tune_batch_size, \n",
    "                                                epochs=config.fine_tune_epochs, \n",
    "                                                validation_data=([valid_data, valid_labels], valid_labels_float), \n",
    "                                                class_weight=dataloader.class_weights,\n",
    "                                                callbacks=[checkpoint])\n",
    "                history_path = postfix + '_history.pkl'\n",
    "\n",
    "                with open(history_path, 'wb') as f:\n",
    "                    pickle.dump(history.history, f)\n",
    "\n",
    "            net.adacos_model.load_weights(best_weight)\n",
    "            adacos_valid_scores = net.adacos_model.evaluate([valid_data, valid_labels], valid_labels_float, verbose=0)\n",
    "            print('Fold #%i validation scores (AdaCos): '%fold, adacos_valid_scores)\n",
    "        \n",
    "        # Convert model to TensorFlow-Lite version\n",
    "        if(net.adacos_model!=None):\n",
    "            tflite_file_path = postfix + '.tflite'\n",
    "            net.adacos_model = Model(inputs=net.adacos_model.inputs[0], outputs=net.adacos_model.get_layer(config.embedding_layer_name).output)\n",
    "        \n",
    "            if(convert_to_tflite):\n",
    "                convert_to_TFLite(net.adacos_model, tflite_file_path)\n",
    "                \n",
    "        # Get eer\n",
    "        eer = test(net.adacos_model, fold, n_sessions=n_sessions)\n",
    "        \n",
    "        # update eer\n",
    "        avg_eer += eer\n",
    "        \n",
    "        if(eer<best_eer):\n",
    "            best_eer=eer\n",
    "            best_fold=fold\n",
    "\n",
    "    # calculate average eer\n",
    "    avg_eer /= config.n_folds\n",
    "\n",
    "    # write test scores to file\n",
    "    scores_path = config.output_folder + dataloader.test_name + '_scores.txt'\n",
    "    \n",
    "    with open(scores_path, \"w\") as f:\n",
    "        file.write(\"Best fold: {:d}\\nBest EER: {:.6f}\\nAverage EER: {:.6f}\".format(best_fold, best_eer,avg_eer)) \n",
    "    print(\"Best fold: %i\\nBest EER: %.6f\\nAverage EER: %.6f\"%(best_fold, best_eer, avg_eer))\n",
    "\n",
    "config = Options().parse()\n",
    "dataloader = DataLoader(config)\n",
    "retrain_softmax   = True\n",
    "retrain_fine_tune = True\n",
    "convert_to_tflite = True\n",
    "\n",
    "for line in config.test_folders:\n",
    "    parts = line.strip().split(' ')\n",
    "    test_folder, n_sessions = parts[0], int(parts[1])\n",
    "    dataloader.load_test_data(test_folder = test_folder)\n",
    "    train(retrain_softmax, retrain_fine_tune, convert_to_tflite, n_sessions=n_sessions)\n",
    "    retrain_softmax   = False\n",
    "    retrain_finetune  = False\n",
    "    convert_to_tflite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras-GPU",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
